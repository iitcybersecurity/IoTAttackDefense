{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FedLearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3it6zT0ojlfg"
      },
      "source": [
        "**FEDERATED LEARNING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9rlmBQTjvK7"
      },
      "source": [
        "**Import relevant packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOh8YxFcjh83"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "import pandas  as pd\n",
        "import io\n",
        "from google.colab import files\n",
        "from imutils import paths\n",
        "from numpy import array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "BESr2TyDj5TR",
        "outputId": "26bd9d81-ddee-49ef-a70a-af582a75aead"
      },
      "source": [
        "uploaded = files.upload()\n",
        "uploaded2 = files.upload()\n",
        "uploaded3 = files.upload()\n",
        "uploaded4 = files.upload()\n",
        "uploaded5 = files.upload()\n",
        "uploaded6 = files.upload()\n",
        "uploaded7 = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5a4be7e9-81fa-4d99-8fe7-82add8ff3dbf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5a4be7e9-81fa-4d99-8fe7-82add8ff3dbf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving d1_1m_0tm.csv to d1_1m_0tm.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2c2da308-f6a0-4def-9910-2449611b4a5a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2c2da308-f6a0-4def-9910-2449611b4a5a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving d2_1m_0tm.csv to d2_1m_0tm.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b78cb86d-dc71-40b4-ad36-83bb488db545\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b78cb86d-dc71-40b4-ad36-83bb488db545\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving d3_1m_0tm.csv to d3_1m_0tm.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bb32b667-93bc-4f32-b4fa-1dd527845de7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bb32b667-93bc-4f32-b4fa-1dd527845de7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving d4_1m_0tm.csv to d4_1m_0tm.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c1094615-91b4-4886-aaf1-2f93513d7d2b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c1094615-91b4-4886-aaf1-2f93513d7d2b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving d5_1m_0tm.csv to d5_1m_0tm.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1109acb3-49fd-4162-ac99-55408597d0f4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1109acb3-49fd-4162-ac99-55408597d0f4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving d6_1m_0tm.csv to d6_1m_0tm.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-621d5dd3-a7ac-41dc-b8ae-12da40aeaebe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-621d5dd3-a7ac-41dc-b8ae-12da40aeaebe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving d7_1m_0tm.csv to d7_1m_0tm.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a34fygJqkr2a"
      },
      "source": [
        "df = pd.read_csv(io.BytesIO(uploaded['d1_1m_0tm.csv']))\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded2['d2_1m_0tm.csv']))\n",
        "df3 = pd.read_csv(io.BytesIO(uploaded3['d3_1m_0tm.csv']))\n",
        "df4 = pd.read_csv(io.BytesIO(uploaded4['d4_1m_0tm.csv']))\n",
        "df5 = pd.read_csv(io.BytesIO(uploaded5['d5_1m_0tm.csv']))\n",
        "df6 = pd.read_csv(io.BytesIO(uploaded6['d6_1m_0tm.csv']))\n",
        "df7 = pd.read_csv(io.BytesIO(uploaded7['d7_1m_0tm.csv']))\n",
        "\n",
        "card = len(df)\n",
        "card2 = len(df2)\n",
        "card3 = len(df3)\n",
        "card4 = len(df4)\n",
        "card5 = len(df5)\n",
        "card6 = len(df6)\n",
        "card7 = len(df7)\n",
        "tot_card = card + card2 + card3 + card4 + card5 + card6 + card7\n",
        "\n",
        "w1 = card/tot_card\n",
        "w2 = card2/tot_card\n",
        "w3 = card3/tot_card\n",
        "w4 = card4/tot_card\n",
        "w5 = card5/tot_card\n",
        "w6 = card6/tot_card\n",
        "w7 = card7/tot_card\n",
        "\n",
        "cols = list(df.columns)\n",
        "cols = [cols[-1]] + cols[:-1]\n",
        "df = df[cols]\n",
        "\n",
        "cols2 = list(df2.columns)\n",
        "cols2 = [cols2[-1]] + cols2[:-1]\n",
        "df2 = df2[cols2]\n",
        "\n",
        "cols3 = list(df3.columns)\n",
        "cols3 = [cols3[-1]] + cols3[:-1]\n",
        "df3 = df3[cols3]\n",
        "\n",
        "cols4 = list(df4.columns)\n",
        "cols4 = [cols4[-1]] + cols4[:-1]\n",
        "df4 = df4[cols4]\n",
        "\n",
        "cols5 = list(df5.columns)\n",
        "cols5 = [cols5[-1]] + cols5[:-1]\n",
        "df5 = df5[cols5]\n",
        "\n",
        "cols6 = list(df6.columns)\n",
        "cols6 = [cols6[-1]] + cols6[:-1]\n",
        "df6 = df6[cols6]\n",
        "\n",
        "cols7 = list(df7.columns)\n",
        "cols7 = [cols7[-1]] + cols7[:-1]\n",
        "df7 = df7[cols7]\n",
        "\n",
        "# drop activity (last) column \n",
        "df = df.iloc[:,:-1]\n",
        "df2 = df2.iloc[:,:-1]\n",
        "df3 = df3.iloc[:,:-1]\n",
        "df4 = df4.iloc[:,:-1]\n",
        "df5 = df5.iloc[:,:-1]\n",
        "df6 = df6.iloc[:,:-1]\n",
        "df7 = df7.iloc[:,:-1]\n",
        "\n",
        "# drop timestamp (first) column\n",
        "df = df.iloc[: , 1:]\n",
        "df2 = df2.iloc[: , 1:]\n",
        "df3 = df3.iloc[: , 1:]\n",
        "df4 = df4.iloc[: , 1:]\n",
        "df5 = df5.iloc[: , 1:]\n",
        "df6 = df6.iloc[: , 1:]\n",
        "df7 = df7.iloc[: , 1:]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiZAzNzJs0q7"
      },
      "source": [
        "**Data preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4qWRRJFsVxm"
      },
      "source": [
        "# split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps):\n",
        "\tX = list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x = sequence[i:end_ix]\n",
        "\t\tX.append(seq_x)\n",
        "\treturn array(X)\n",
        " \n",
        "values = df.values\n",
        "values2 = df2.values\n",
        "values3 = df3.values\n",
        "values4 = df4.values\n",
        "values5 = df5.values\n",
        "values6 = df6.values\n",
        "values7 = df7.values\n",
        "# choose a number of time steps\n",
        "n_steps = 16\n",
        "# split into samples\n",
        "X = split_sequence(values, n_steps)\n",
        "X2 = split_sequence(values2, n_steps)\n",
        "X3 = split_sequence(values3, n_steps)\n",
        "X4 = split_sequence(values4, n_steps)\n",
        "X5 = split_sequence(values5, n_steps)\n",
        "X6 = split_sequence(values6, n_steps)\n",
        "X7 = split_sequence(values7, n_steps)\n",
        "\n",
        "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "n_features = 29\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
        "X2 = X2.reshape((X2.shape[0], X2.shape[1], n_features))\n",
        "X3 = X3.reshape((X3.shape[0], X3.shape[1], n_features))\n",
        "X4 = X4.reshape((X4.shape[0], X4.shape[1], n_features))\n",
        "X5 = X5.reshape((X5.shape[0], X5.shape[1], n_features))\n",
        "X6 = X6.reshape((X6.shape[0], X6.shape[1], n_features))\n",
        "X7 = X7.reshape((X7.shape[0], X7.shape[1], n_features))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZUDkL4Qnf5J"
      },
      "source": [
        "**Split data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXLHfMZEnfdH"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def split_dataset(X):\n",
        "    #we perform a 80:20 split of the dataset in conjunction with a randomization \n",
        "    #(shuffling the dataset is important to have a good training and a good generalization)\n",
        "    X_train, X_test = train_test_split(X, test_size=0.20, random_state=42)\n",
        "    return X_train, X_test"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gbK7Nv2cDQ5"
      },
      "source": [
        "**Model building**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIfETkSD0AGZ"
      },
      "source": [
        "def model_creation(steps, features):\n",
        "  k_size = 5\n",
        "  model = keras.Sequential(\n",
        "    [\n",
        "        layers.Input(shape=(steps, features)),\n",
        "        layers.Conv1D(filters=16, kernel_size=k_size, padding=\"same\", strides=2, activation=\"relu\"),\n",
        "        #layers.Dropout(rate=0.2),\n",
        "        layers.Conv1D(filters=8, kernel_size=k_size, padding=\"same\", strides=2, activation=\"relu\"),\n",
        "        layers.Conv1DTranspose(filters=8, kernel_size=k_size, padding=\"same\", strides=2, activation=\"relu\"),\n",
        "        #layers.Dropout(rate=0.2),\n",
        "        layers.Conv1DTranspose(filters=16, kernel_size=k_size, padding=\"same\", strides=2, activation=\"relu\"),\n",
        "        layers.Conv1DTranspose(filters=29, kernel_size=k_size, padding=\"same\"),\n",
        "    ])\n",
        "  return model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKQg065hq0O3"
      },
      "source": [
        "def model_fitting(model, train_data):\n",
        "  model.fit(train_data, train_data, epochs=30, batch_size=256,validation_split=0.2,\n",
        "            callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")],\n",
        "    )"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm6jYR77tflB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51ebde5c-de1d-43ee-f389-94d9a83d347e"
      },
      "source": [
        "X_train, X_test = split_dataset(X)\n",
        "X_train2, X_test2 = split_dataset(X2)\n",
        "X_train3, X_test3 = split_dataset(X3)\n",
        "X_train4, X_test4 = split_dataset(X4)\n",
        "X_train5, X_test5 = split_dataset(X5)\n",
        "X_train6, X_test6 = split_dataset(X6)\n",
        "X_train7, X_test7 = split_dataset(X7)\n",
        "\n",
        "m1 = model_creation(n_steps, n_features)\n",
        "m1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "model_fitting(m1, X_train)\n",
        "\n",
        "m2 = model_creation(n_steps, n_features)\n",
        "m2.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "model_fitting(m2, X_train2)\n",
        "\n",
        "m3 = model_creation(n_steps, n_features)\n",
        "m3.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "model_fitting(m3, X_train3)\n",
        "\n",
        "m4 = model_creation(n_steps, n_features)\n",
        "m4.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "model_fitting(m4, X_train4)\n",
        "\n",
        "m5 = model_creation(n_steps, n_features)\n",
        "m5.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "model_fitting(m5, X_train5)\n",
        "\n",
        "m6 = model_creation(n_steps, n_features)\n",
        "m6.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "model_fitting(m6, X_train6)\n",
        "\n",
        "m7 = model_creation(n_steps, n_features)\n",
        "m7.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "model_fitting(m7, X_train7)\n",
        "\n",
        "#history = m1.fit(X_train, X_train, epochs=5, batch_size=128, validation_split=0.2,\n",
        "#    callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")],\n",
        "#)\n",
        "\n",
        "#m1.summary()\n",
        "#pyplot.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "#pyplot.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "#pyplot.legend()\n",
        "#pyplot.show()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Epoch 1/30\n",
            "47/47 [==============================] - 2s 21ms/step - loss: 0.1379 - val_loss: 0.0892\n",
            "Epoch 2/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0742 - val_loss: 0.0600\n",
            "Epoch 3/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0477 - val_loss: 0.0364\n",
            "Epoch 4/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0310 - val_loss: 0.0253\n",
            "Epoch 5/30\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 0.0225 - val_loss: 0.0199\n",
            "Epoch 6/30\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 0.0189 - val_loss: 0.0174\n",
            "Epoch 7/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0165 - val_loss: 0.0149\n",
            "Epoch 8/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0136 - val_loss: 0.0120\n",
            "Epoch 9/30\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 0.0115 - val_loss: 0.0107\n",
            "Epoch 10/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0105 - val_loss: 0.0098\n",
            "Epoch 11/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0096 - val_loss: 0.0090\n",
            "Epoch 12/30\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 0.0087 - val_loss: 0.0081\n",
            "Epoch 13/30\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 0.0078 - val_loss: 0.0074\n",
            "Epoch 14/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0072 - val_loss: 0.0068\n",
            "Epoch 15/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0067 - val_loss: 0.0063\n",
            "Epoch 16/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0062 - val_loss: 0.0060\n",
            "Epoch 17/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0059 - val_loss: 0.0057\n",
            "Epoch 18/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0056 - val_loss: 0.0054\n",
            "Epoch 19/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0054 - val_loss: 0.0052\n",
            "Epoch 20/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0052 - val_loss: 0.0050\n",
            "Epoch 21/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0050 - val_loss: 0.0049\n",
            "Epoch 22/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0049 - val_loss: 0.0047\n",
            "Epoch 23/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0047 - val_loss: 0.0046\n",
            "Epoch 24/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0046 - val_loss: 0.0045\n",
            "Epoch 25/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0045 - val_loss: 0.0044\n",
            "Epoch 26/30\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 27/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 28/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0043 - val_loss: 0.0042\n",
            "Epoch 29/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 30/30\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Epoch 1/30\n",
            "94/94 [==============================] - 2s 18ms/step - loss: 0.0913 - val_loss: 0.0478\n",
            "Epoch 2/30\n",
            "94/94 [==============================] - 2s 17ms/step - loss: 0.0328 - val_loss: 0.0241\n",
            "Epoch 3/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0215 - val_loss: 0.0179\n",
            "Epoch 4/30\n",
            "94/94 [==============================] - 2s 17ms/step - loss: 0.0156 - val_loss: 0.0134\n",
            "Epoch 5/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0122 - val_loss: 0.0110\n",
            "Epoch 6/30\n",
            "94/94 [==============================] - 2s 17ms/step - loss: 0.0102 - val_loss: 0.0093\n",
            "Epoch 7/30\n",
            "94/94 [==============================] - 1s 16ms/step - loss: 0.0088 - val_loss: 0.0081\n",
            "Epoch 8/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0077 - val_loss: 0.0071\n",
            "Epoch 9/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0068 - val_loss: 0.0063\n",
            "Epoch 10/30\n",
            "94/94 [==============================] - 1s 16ms/step - loss: 0.0060 - val_loss: 0.0057\n",
            "Epoch 11/30\n",
            "94/94 [==============================] - 1s 16ms/step - loss: 0.0055 - val_loss: 0.0053\n",
            "Epoch 12/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0052 - val_loss: 0.0050\n",
            "Epoch 13/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0049 - val_loss: 0.0047\n",
            "Epoch 14/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 15/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 16/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0043 - val_loss: 0.0042\n",
            "Epoch 17/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 18/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 19/30\n",
            "94/94 [==============================] - 1s 16ms/step - loss: 0.0040 - val_loss: 0.0039\n",
            "Epoch 20/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 21/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0038 - val_loss: 0.0037\n",
            "Epoch 22/30\n",
            "94/94 [==============================] - 2s 17ms/step - loss: 0.0037 - val_loss: 0.0036\n",
            "Epoch 23/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0037 - val_loss: 0.0036\n",
            "Epoch 24/30\n",
            "94/94 [==============================] - 2s 17ms/step - loss: 0.0036 - val_loss: 0.0035\n",
            "Epoch 25/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0035 - val_loss: 0.0034\n",
            "Epoch 26/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0035 - val_loss: 0.0034\n",
            "Epoch 27/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0034 - val_loss: 0.0033\n",
            "Epoch 28/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 29/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 30/30\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Epoch 1/30\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.0875 - val_loss: 0.0613\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0441 - val_loss: 0.0337\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0293 - val_loss: 0.0243\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0220 - val_loss: 0.0197\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0188 - val_loss: 0.0174\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0169 - val_loss: 0.0157\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0152 - val_loss: 0.0140\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0134 - val_loss: 0.0125\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0122 - val_loss: 0.0115\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0113 - val_loss: 0.0108\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0106 - val_loss: 0.0101\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0100 - val_loss: 0.0096\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.0095 - val_loss: 0.0092\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0091 - val_loss: 0.0090\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.0089 - val_loss: 0.0087\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0087 - val_loss: 0.0085\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0085 - val_loss: 0.0083\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0083 - val_loss: 0.0082\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0082 - val_loss: 0.0081\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.0081 - val_loss: 0.0079\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.0079 - val_loss: 0.0079\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.0078 - val_loss: 0.0077\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.0077 - val_loss: 0.0077\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.0076 - val_loss: 0.0076\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0076 - val_loss: 0.0075\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0075 - val_loss: 0.0074\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0074 - val_loss: 0.0074\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0074 - val_loss: 0.0073\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0073 - val_loss: 0.0072\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.0072 - val_loss: 0.0072\n",
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Epoch 1/30\n",
            "101/101 [==============================] - 2s 18ms/step - loss: 0.1064 - val_loss: 0.0491\n",
            "Epoch 2/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0388 - val_loss: 0.0301\n",
            "Epoch 3/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0228 - val_loss: 0.0194\n",
            "Epoch 4/30\n",
            "101/101 [==============================] - 2s 17ms/step - loss: 0.0175 - val_loss: 0.0166\n",
            "Epoch 5/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0152 - val_loss: 0.0142\n",
            "Epoch 6/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0130 - val_loss: 0.0123\n",
            "Epoch 7/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0115 - val_loss: 0.0110\n",
            "Epoch 8/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0103 - val_loss: 0.0099\n",
            "Epoch 9/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0093 - val_loss: 0.0089\n",
            "Epoch 10/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0084 - val_loss: 0.0082\n",
            "Epoch 11/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0078 - val_loss: 0.0076\n",
            "Epoch 12/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0072 - val_loss: 0.0071\n",
            "Epoch 13/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0068 - val_loss: 0.0068\n",
            "Epoch 14/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0065 - val_loss: 0.0064\n",
            "Epoch 15/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0062 - val_loss: 0.0062\n",
            "Epoch 16/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0060 - val_loss: 0.0060\n",
            "Epoch 17/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0058 - val_loss: 0.0057\n",
            "Epoch 18/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0055 - val_loss: 0.0055\n",
            "Epoch 19/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0054 - val_loss: 0.0054\n",
            "Epoch 20/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0052 - val_loss: 0.0052\n",
            "Epoch 21/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0051 - val_loss: 0.0051\n",
            "Epoch 22/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0049 - val_loss: 0.0050\n",
            "Epoch 23/30\n",
            "101/101 [==============================] - 2s 17ms/step - loss: 0.0048 - val_loss: 0.0049\n",
            "Epoch 24/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0047 - val_loss: 0.0047\n",
            "Epoch 25/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0046 - val_loss: 0.0046\n",
            "Epoch 26/30\n",
            "101/101 [==============================] - 2s 17ms/step - loss: 0.0045 - val_loss: 0.0045\n",
            "Epoch 27/30\n",
            "101/101 [==============================] - 2s 16ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 28/30\n",
            "101/101 [==============================] - 2s 17ms/step - loss: 0.0043 - val_loss: 0.0043\n",
            "Epoch 29/30\n",
            "101/101 [==============================] - 2s 17ms/step - loss: 0.0042 - val_loss: 0.0042\n",
            "Epoch 30/30\n",
            "101/101 [==============================] - 2s 17ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Epoch 1/30\n",
            "70/70 [==============================] - 2s 19ms/step - loss: 0.1429 - val_loss: 0.0944\n",
            "Epoch 2/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0697 - val_loss: 0.0489\n",
            "Epoch 3/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0418 - val_loss: 0.0374\n",
            "Epoch 4/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0330 - val_loss: 0.0292\n",
            "Epoch 5/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0259 - val_loss: 0.0234\n",
            "Epoch 6/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0215 - val_loss: 0.0202\n",
            "Epoch 7/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0190 - val_loss: 0.0181\n",
            "Epoch 8/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0169 - val_loss: 0.0157\n",
            "Epoch 9/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0146 - val_loss: 0.0138\n",
            "Epoch 10/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0131 - val_loss: 0.0126\n",
            "Epoch 11/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0121 - val_loss: 0.0118\n",
            "Epoch 12/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0115 - val_loss: 0.0113\n",
            "Epoch 13/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0110 - val_loss: 0.0108\n",
            "Epoch 14/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0105 - val_loss: 0.0103\n",
            "Epoch 15/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0101 - val_loss: 0.0100\n",
            "Epoch 16/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0098 - val_loss: 0.0096\n",
            "Epoch 17/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0094 - val_loss: 0.0093\n",
            "Epoch 18/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0091 - val_loss: 0.0089\n",
            "Epoch 19/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0088 - val_loss: 0.0087\n",
            "Epoch 20/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0086 - val_loss: 0.0085\n",
            "Epoch 21/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0084 - val_loss: 0.0083\n",
            "Epoch 22/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0082 - val_loss: 0.0081\n",
            "Epoch 23/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0080 - val_loss: 0.0079\n",
            "Epoch 24/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0078 - val_loss: 0.0078\n",
            "Epoch 25/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0077 - val_loss: 0.0076\n",
            "Epoch 26/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0076 - val_loss: 0.0075\n",
            "Epoch 27/30\n",
            "70/70 [==============================] - 1s 18ms/step - loss: 0.0074 - val_loss: 0.0073\n",
            "Epoch 28/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0073 - val_loss: 0.0073\n",
            "Epoch 29/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0072 - val_loss: 0.0071\n",
            "Epoch 30/30\n",
            "70/70 [==============================] - 1s 17ms/step - loss: 0.0071 - val_loss: 0.0071\n",
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Epoch 1/30\n",
            "205/205 [==============================] - 4s 18ms/step - loss: 0.0874 - val_loss: 0.0332\n",
            "Epoch 2/30\n",
            "205/205 [==============================] - 4s 17ms/step - loss: 0.0226 - val_loss: 0.0161\n",
            "Epoch 3/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0126 - val_loss: 0.0104\n",
            "Epoch 4/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0091 - val_loss: 0.0077\n",
            "Epoch 5/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0070 - val_loss: 0.0061\n",
            "Epoch 6/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0056 - val_loss: 0.0050\n",
            "Epoch 7/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0047 - val_loss: 0.0043\n",
            "Epoch 8/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0041 - val_loss: 0.0037\n",
            "Epoch 9/30\n",
            "205/205 [==============================] - 3s 16ms/step - loss: 0.0036 - val_loss: 0.0033\n",
            "Epoch 10/30\n",
            "205/205 [==============================] - 3s 16ms/step - loss: 0.0032 - val_loss: 0.0030\n",
            "Epoch 11/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0030 - val_loss: 0.0028\n",
            "Epoch 12/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 13/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 14/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 15/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0022 - val_loss: 0.0021\n",
            "Epoch 16/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0021 - val_loss: 0.0020\n",
            "Epoch 17/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 18/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0019 - val_loss: 0.0018\n",
            "Epoch 19/30\n",
            "205/205 [==============================] - 3s 16ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 20/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 21/30\n",
            "205/205 [==============================] - 3s 16ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 22/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 23/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 24/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 25/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 26/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 27/30\n",
            "205/205 [==============================] - 3s 16ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 28/30\n",
            "205/205 [==============================] - 3s 16ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 29/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 30/30\n",
            "205/205 [==============================] - 3s 17ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Epoch 1/30\n",
            "124/124 [==============================] - 3s 19ms/step - loss: 0.0904 - val_loss: 0.0425\n",
            "Epoch 2/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0301 - val_loss: 0.0215\n",
            "Epoch 3/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0163 - val_loss: 0.0129\n",
            "Epoch 4/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0109 - val_loss: 0.0094\n",
            "Epoch 5/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0084 - val_loss: 0.0077\n",
            "Epoch 6/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0070 - val_loss: 0.0066\n",
            "Epoch 7/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0061 - val_loss: 0.0058\n",
            "Epoch 8/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0054 - val_loss: 0.0051\n",
            "Epoch 9/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0048 - val_loss: 0.0046\n",
            "Epoch 10/30\n",
            "124/124 [==============================] - 2s 16ms/step - loss: 0.0043 - val_loss: 0.0042\n",
            "Epoch 11/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0040 - val_loss: 0.0039\n",
            "Epoch 12/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0037 - val_loss: 0.0036\n",
            "Epoch 13/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0034 - val_loss: 0.0033\n",
            "Epoch 14/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0032 - val_loss: 0.0031\n",
            "Epoch 15/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0030 - val_loss: 0.0029\n",
            "Epoch 16/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0028 - val_loss: 0.0027\n",
            "Epoch 17/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 18/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 19/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 20/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 21/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 22/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 23/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0021 - val_loss: 0.0021\n",
            "Epoch 24/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0021 - val_loss: 0.0021\n",
            "Epoch 25/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0020 - val_loss: 0.0021\n",
            "Epoch 26/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 27/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 28/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 29/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0019 - val_loss: 0.0019\n",
            "Epoch 30/30\n",
            "124/124 [==============================] - 2s 17ms/step - loss: 0.0019 - val_loss: 0.0019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfVejIzHzWtj"
      },
      "source": [
        "**Computing Average Weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lR_6Q1MRqQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d608e2b-d2e7-4aae-abfb-438c3c0b8f2e"
      },
      "source": [
        "weights1 = m1.get_weights()\n",
        "weights2 = m2.get_weights()\n",
        "weights3 = m3.get_weights()\n",
        "weights4 = m4.get_weights()\n",
        "weights5 = m5.get_weights()\n",
        "weights6 = m6.get_weights()\n",
        "weights7 = m7.get_weights()\n",
        "lun = len(weights1)\n",
        "#print(\"Pesi:\", weights1)\n",
        "#print(\"Pesi:\", weights2)\n",
        "avg_weights = list()\n",
        "\n",
        "for i in range(lun):\n",
        "    temp_weights = (weights1[i]*w1 + weights2[i]*w2 + weights3[i]*w3 + weights4[i]*w4 + weights5[i]*w5 + weights6[i]*w6 + weights7[i]*w7)\n",
        "    avg_weights.append(temp_weights)\n",
        "\n",
        "#print(\"avg_weights:\", avg_weights)\n",
        "\n",
        "global_model = model_creation(n_steps, n_features)\n",
        "global_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "global_model.set_weights(avg_weights)\n",
        "global_model.summary()\n",
        "#dopo TEST"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_46 (Conv1D)           (None, 8, 16)             2336      \n",
            "_________________________________________________________________\n",
            "conv1d_47 (Conv1D)           (None, 4, 8)              648       \n",
            "_________________________________________________________________\n",
            "conv1d_transpose_69 (Conv1DT (None, 8, 8)              328       \n",
            "_________________________________________________________________\n",
            "conv1d_transpose_70 (Conv1DT (None, 16, 16)            656       \n",
            "_________________________________________________________________\n",
            "conv1d_transpose_71 (Conv1DT (None, 16, 29)            2349      \n",
            "=================================================================\n",
            "Total params: 6,317\n",
            "Trainable params: 6,317\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSt_qQR6RsGJ"
      },
      "source": [
        "**Prepare test data and detect anomalies (446)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "cG9RPn3XTPPT",
        "outputId": "c4684852-9853-44be-bd81-9378b3403161"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded_an = files.upload()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-373208ae-4c4d-4cc6-8f99-04d4c9e4a55a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-373208ae-4c4d-4cc6-8f99-04d4c9e4a55a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving merge_an_0tm.csv to merge_an_0tm.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "dJwXcJzDxixr",
        "outputId": "1b5b2f74-ce20-46fa-e99f-671e5d7d1f9b"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded_merge = files.upload()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b91b37fe-a908-4d00-b36b-5ff64188bc34\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b91b37fe-a908-4d00-b36b-5ff64188bc34\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving merge_0tm.csv to merge_0tm.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2n2QTDyAAKC"
      },
      "source": [
        "df_an = pd.read_csv(io.BytesIO(uploaded_an['merge_an_0tm.csv']))\n",
        "\n",
        "cols_an = list(df_an.columns)\n",
        "cols_an = [cols_an[-1]] + cols_an[:-1]\n",
        "df_an = df_an[cols_an]\n",
        "\n",
        "activity = df_an.iloc[:,-1]\n",
        "cardinality = activity.shape[0]\n",
        "\n",
        "anomaly_real = list()\n",
        "for items in activity.iteritems():\n",
        "  if 'ano' in items[1]:\n",
        "    anomaly_real.append(items[0])\n",
        "\n",
        "number_anomaly = len(anomaly_real)\n",
        "number_ok = cardinality - number_anomaly\n",
        "#for i in range(len(index)):\n",
        "  #print(index[i])\n",
        "\n",
        "# drop activity (last) column \n",
        "df_an = df_an.iloc[:,:-1]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "df_an['morning'] = 0\n",
        "df_an['Dates'] = pd.to_datetime(df_an['timestamp']).dt.date\n",
        "df_an['Time'] = pd.to_datetime(df_an['timestamp']).dt.time\n",
        "\n",
        "date1 = time(18, 00, 00)\n",
        "df_an.loc[df_an['Time'] >= date1, 'morning'] = 1\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# drop timestamp (first) column\n",
        "df_an = df_an.iloc[: , 1:]\n",
        "#df_an = df_an.iloc[:,:-1]\n",
        "#df_an = df_an.iloc[:,:-1]\n",
        "\n",
        "#print(df_an.head())\n",
        "\n",
        "values_an = df_an.values"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRTFvlmrxk5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f49a0fb-3493-4161-cb50-9c4c6727195f"
      },
      "source": [
        "df_merge = pd.read_csv(io.BytesIO(uploaded_merge['merge_0tm.csv']))\n",
        "\n",
        "cols_merge = list(df_merge.columns)\n",
        "\n",
        "cols_merge = [cols_merge[-1]] + cols_merge[:-1]\n",
        "df_merge = df_merge[cols_merge]\n",
        "\n",
        "# drop activity (last) column\n",
        "df_merge = df_merge.iloc[:,:-1]\n",
        "\n",
        "# Order dataset by timestamp\n",
        "#df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "#df['timestamp'].min(), df['timestamp'].max()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "df_merge['morning'] = 0\n",
        "df_merge['Dates'] = pd.to_datetime(df_merge['timestamp']).dt.date\n",
        "df_merge['Time'] = pd.to_datetime(df_merge['timestamp']).dt.time\n",
        "  \n",
        "date1 = time(18, 00, 00)\n",
        "df_merge.loc[df_merge['Time'] >= date1, 'morning'] = 1\n",
        "df_merge = df_merge.iloc[:,:-1]\n",
        "df_merge = df_merge.iloc[:,:-1]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# drop timestamp (first) column\n",
        "df_merge = df_merge.iloc[: , 1:]\n",
        "\n",
        "#aggiungere colonna attività e attività buona o anomala\n",
        "print(df_merge.head())\n",
        "\n",
        "values_merge = df_merge.values\n",
        "\n",
        "X_merge = split_sequence(values_merge, n_steps)\n",
        "X_merge = X_merge.reshape((X_merge.shape[0], X_merge.shape[1], n_features))\n",
        "X_train_merge, X_test_merge = split_dataset(X_merge)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   wardrobe  tv  oven  ...  bathroomDoorLock  bathroomDoor  bathroomCarp\n",
            "0         0   0     0  ...                 0             0             0\n",
            "1         0   0     0  ...                 0             0             0\n",
            "2         0   0     0  ...                 0             0             0\n",
            "3         0   0     0  ...                 0             0             0\n",
            "4         0   0     0  ...                 0             0             0\n",
            "\n",
            "[5 rows x 29 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lG0yTz9Erkb"
      },
      "source": [
        "**Threshold**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-j7aJGkqyZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79cb688-e265-48c5-c89d-a4dba012ba64"
      },
      "source": [
        "# Get train MAE loss\n",
        "X_train_pred = global_model.predict(X_train_merge)\n",
        "train_mae_loss = np.mean(np.abs(X_train_pred - X_train_merge), axis=1)\n",
        "\n",
        "#pyplot.hist(train_mae_loss, bins=50)\n",
        "#pyplot.xlabel(\"Train MAE loss\")\n",
        "#pyplot.ylabel(\"No of samples\")\n",
        "#pyplot.show()\n",
        "\n",
        "# Get reconstruction loss threshold\n",
        "threshold = np.max(train_mae_loss)\n",
        "print(\"Reconstruction error threshold: \", threshold)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reconstruction error threshold:  1.0689212400466204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5bXZfxXRvvL"
      },
      "source": [
        "X_an = split_sequence(values_an, n_steps)\n",
        "\n",
        "# Get test MAE loss.\n",
        "X_an_pred = global_model.predict(X_an)\n",
        "test_mae_loss_an = np.mean(np.abs(X_an_pred - X_an), axis=1)\n",
        "#test_mae_loss_an = test_mae_loss_an.reshape((-1))\n",
        "\n",
        "#pyplot.hist(test_mae_loss_an, bins=50)\n",
        "#pyplot.xlabel(\"test MAE loss\")\n",
        "#pyplot.ylabel(\"No of samples\")\n",
        "#pyplot.show()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2gMDy8M61gk",
        "outputId": "4df2364b-036c-4579-af74-831b02481cbc"
      },
      "source": [
        "th = threshold*0.95\n",
        "# Detect all the samples which are anomalies.\n",
        "#anomalies = test_mae_loss_an > threshold\n",
        "anomalies = test_mae_loss_an > th\n",
        "\n",
        "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
        "#print(\"Indices of anomaly samples: \", np.where(anomalies))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of anomaly samples:  176329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1BN7CS2pya_"
      },
      "source": [
        "**Compute Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EPwuCYYp2iH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e475c04f-3358-4f36-db94-cfcc60629030"
      },
      "source": [
        "anomaly_detected = np.where(anomalies)[0]\n",
        "#remove duplicates\n",
        "anomaly_detected = list(dict.fromkeys(anomaly_detected))\n",
        "TP = 0\n",
        "FP = 0\n",
        "TN = 0\n",
        "FN = 0\n",
        "for ind in anomaly_detected:\n",
        "  if ind in anomaly_real:\n",
        "    TP = TP+1\n",
        "  else:\n",
        "    FP = FP+1\n",
        "for ind2 in anomaly_real:\n",
        "  if ind2 not in anomaly_detected:\n",
        "    FN = FN+1\n",
        "TN = cardinality - TP - FP - FN \n",
        "print(\"TP:\", TP, \"\\nTN:\", TN, \"\\nFP:\", FP, \"\\nFN:\", FN)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TP: 34 \n",
            "TN: 164548 \n",
            "FP: 142665 \n",
            "FN: 412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roP7yFQO8APC"
      },
      "source": [
        "**Compute Indexes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqngGCyA8D0x",
        "outputId": "d8553295-36f4-4721-e697-76a481960d74"
      },
      "source": [
        "def truncate(f, n):\n",
        "    '''Truncates/pads a float f to n decimal places without rounding'''\n",
        "    s = '{}'.format(f)\n",
        "    if 'e' in s or 'E' in s:\n",
        "        return '{0:.{1}f}'.format(f, n)\n",
        "    i, p, d = s.partition('.')\n",
        "    return '.'.join([i, (d+'0'*n)[:n]])\n",
        "\n",
        "accuracy = (TP+TN)/cardinality\n",
        "accuracy = truncate(accuracy, 3)\n",
        "\n",
        "precision = TP/(TP+FP)\n",
        "precision = truncate(precision, 3)\n",
        "\n",
        "sensitivity = TP/(TP+FN)\n",
        "sensitivity = truncate(sensitivity, 3)\n",
        "\n",
        "specificity = TN/(TN+FP)\n",
        "specificity = truncate(specificity, 3)\n",
        "\n",
        "print(\"Accuracy:\", accuracy, \"\\nPrecision:\", precision, \"\\nSensitivity:\", sensitivity, \"\\nSpecificity:\", specificity)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.534 \n",
            "Precision: 0.000 \n",
            "Sensitivity: 0.076 \n",
            "Specificity: 0.535\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}